## 摘要

深度神经网络是一种高度表达的模型，并且在最近在语音和视觉识别任务上取得了最佳性能效果。尽管他们的表现力是他们成功的原因，但也造成他们可能学习到具有反直觉属性的难以解释的解决方法。在本文中，我们报告了两种这样的属性。

首先我们通过一系列的单元分析方法发现在各个高级单元和高级单元的随即线性组合之间没有区别。这说明，其实是空间而非是单个神经元保留了神经网络的高维层次结构的语义信息。

其次，我们发现深度神经网络学习到的输入-输出映射在很大程度上是不连续的。我们可以通过应用某些难以感知的扰动使网络对图像进行错误的分类，这些扰动使通过最大化网络的预测误差发现的。除此之外，这些扰动的特定性质不是学习的随机产物，相同的扰动会导致在数据集的不同子集上训练的不同网络对相同输入进行错误的分类。

## 1.引言

深度神经网络使功能强大的学习模型，可以在视觉和语音识别问题上表现出出色的性能。神经网络之所以可以达到高性能是因为它们可以表达任意的计算，这些计算由适量的大量的并行非线性步骤组成。但是由于结果计算是通过监督学习的反向传播机制发现的，因此可能很难解释并且拥有反直觉的特性。（注：这里说的反直觉特性指的是网络，而不是指的反向传播机制）在本文中我们讨论了深度神经网络两个反直觉特性。

第一个属性与单个单元的语义有关。先前的工作通过找到最大程度地激活给定单元的输入集来分析各种单元的语义含义。对单个单元的检查隐含了一个假设，即最后一个特征层的单元形成了一个可区分的基础，这对于提取语义信息特别有用。我们将在第三部分证明$\phi(x)$的随机投影和$\phi(x)$的坐标在语义上是不可区分的，这使人们对神经网络解开坐标间变化因素的猜想提出了质疑。一般来说，是整个激活的空间而不是单个的神经元包含了语义信息。Mikolov等人最近对单词的表示得出了一个相似但强的结论，他们发现单词的表征在矢量空间的各个方向上拥有令人惊讶的丰富的语义编码的关系和类比。同时，向量表示在空间旋转之前都是稳定的，因此矢量表示的各个单元不太可能包含语义信息。

第二个属性和神经网络对输入的微扰的稳定性相关。考虑一个性能优异的深度神经网络，它的泛化性能优秀并且在物体识别任务上取得良好性能。我们期望这样的神经网络能在输入由微小扰动的情况下依然具备鲁棒性，因为小的扰动不会图像的对象类别。但是我们发现将不可察觉的非随机扰动应用于测试图像后，就可以任意地干预到网络的预测结果。这些扰动是通过优化输入后最大化预测误差得到的，我们把这些干扰的样本称为“对抗性样本”。

我们很自然地期望最小的必要干扰的精确配置实在反向传播学习中不同的运行过程下出现的正常变异性的随机伪像。然而我们发现对抗性样本是相当鲁棒的，并且在具有不同层数、激活次数或在训练数据的子集上训练的神经网络上都能干扰。也就是说，如果我们使用一个神经网络生成一组对抗样本，则会发现这些样本对于另一个神经网络在统计学的角度上看依然被很难被正确分类，尽管其他的神经网络使用了不同的超参数、不同的样本数据训练。

这些结果表明，通过反向传播学习的神经网络具有非直觉特性和固有盲点，其结构与数据分布相关，只不过这种分布不能被显示地观察到。

## 2.框架

定义： 我们使用$x\in\mathbb{R}^m$表示输入图像，$\phi(x)$表示某层的激活值。我们首先检查$\phi(x)$的属性，然后搜索其盲点。我们在不同的网络上对三个数据集进行了实验：

* 对于`MNIST`数据集，我们使用了如下的配置：
  * 具有一个或多个隐藏层和softmax的分类器的简单的全连接网络。我们称其为`FC`。
  * 一个自动编码器上训练的分类器，我们称其为`AE`。
* `ImageNet`数据集：
  * 使用了Krizhevsky的架构。我们称其为\`AlexNet`。
* ~10M的`Youtube`样本：
  * 无监督的训练网络，大约有10亿可学习参数。我们称其为`QuocNet`。

对于`MNIST`数据集上的实验，我们使用了权重衰减为$\lambda$的正则化方法。此外，在一些实验中，我们将`MNIST`上训练的数据集分为两个子集$P_1$和$P_2$，每一个子集都有30,000个样本。

## 3.$\phi(x)$的单元

传统的计算机视觉系统依赖于特征提取：通常单个特征很容易解释，比如说颜色的直方图或量化的局部导数。这允许研究者去检查特征空间中的每一个体的坐标，并将它们与语义丰富的原始图像进行连接。先前的工作在分析应用神经网络解决计算机视觉问题时使用了相似的机理。这些工作将一个激活的隐藏层单元作为一个有意义的特征，他们寻找一种一种使该单一特征激活值最大化的输入图形。

前面提及的这种技术可以正式表述为图像$x'$的视觉检查，这些图像满足(或接近最大可到达值)：
$$
\begin{equation}
x'={\arg\max_{x\in mathcal{I}}}<\phi(x),e_i>
\end{equation}
$$
其中，$I$为未接受网络训练的数据分布的图像集，$e_i$为第$i$个隐藏单元关联的自然基向量。我们的实验表明，任何随机方向$v\in\mathbb{R}^n$都会产生相似的可解释性的语义属性。更正式地说，我们发现图像$x'$在语义上彼此相关，对于许多$x'$:
$$
\begin{equation}
x'=\arg\max_{x\in\mathcal{I}}<\phi(x),v>
\end{equation}
$$
这说明对于检查$\phi(x)$的属性而言，自然基向量并不会比随机的基向量效果更好。这使人们对神经网络解开坐标间变化因素的猜想提出了质疑。（注：我也没看太懂，原文是’This puts into question the notion that neural networks disentangle variation factors across coordinates.‘）

首先，我们在`MNIST`数据集上训练卷积神经网络以评估我们上述的想法。我们使用MNIST的测试集作为$\mathcal{I}$。图1展示了自然基础上的最大化的激活图像，图2展示了在随机方向上的最大化激活图像。在这两种情况下，两种图像都有许多高层的相似性。

接着我们在`AlexNet`上重复了我们的实验，我们使用了验证集作为$\mathcal{I}$。图3和图4对比了训练网络上的随机和自然基向量。对于单个单元以及单元的组合，每行所展示的语义似乎都是有意义的。

尽管这种分析提供了关于$\phi$在输入分布的特定子集上生成不变性的能力的见解，但它并未解释其其余域的行为。在下一节中，我们将看到$\phi$在几乎每个点形式的数据分布附近都具有违反直觉的特性。

## 神经网络的盲点

到目前为止，除了确认有关由深度神经网络学习的表示的复杂性的某些直觉之外，单元级检查方法的实用性相对较小。全局的网络级别检查方法在解释模型做出的分类决策时可能很有用，并且可以用于例如识别导致对给定视觉输入实例进行正确分类的输入部分。换句话说，可以使用经过训练的模型进行弱监督定位。这样的全局分析很有用，因为它们可以使我们更好地理解受过训练的网络所代表的输入到输出映射。

一般来说，神经网络的输出层单元是其输入的高度非线性函数。当使用交叉熵损失对其进行训练时（使用Softmax激活函数），它表示给定输入（以及到目前为止提供的训练集）的标签的条件分布。有人认为，在神经网络的输入和输出单元之间的非线性层的深层堆栈是模型在输入空间上编码非局部泛化先验的一种方法。换句话说，假设输出单元可以为输入空间的各个区域分配非重要（可能是非$\varepsilon$）概率，而未训练的样本就在这个邻域中。例如，这些区域是可以从不同的角度对相同的对象及逆行表示，这些对象在像素空间中相对较远，但是以他们共享原始输入的标签和统计特性。

如上的想法意味着局部的泛化在训练过程中可以如期地工作。尤其是在给定训练输入$x$时添加的一个足够小的半径$\varepsilon>0$(注：这个半径我的理解是某些语义弱相关的信息或者无关信息的延伸)，满足$||r||<\varepsilon$的的样本$x+r$将会被模型输出到一个较高的正确类别的概率。这种平滑的先验通常对计算机视觉的问题有效，因为通常来说，给定图像细微的扰动不会改变其基础的类别。

我们的主要结果是，对于深度的神经网络，许多核方法在平滑假设阶段就不成立了。具体而言，我们通过使用简单的优化方法找到了对抗样本，这些样本时通过对正确分类的输入图像进行不明显的微小扰动获得的，所以将不再被正确分类。

从某种意义上说，我们所描述的是一种以有效方式（通过优化）遍历网络所表示的流形(注：Manifolds)并在输入空间中找到对抗样本的方法。对抗样本代表的就是流形中的低概率（但是高维的）的”口袋“，他们很难仅通过围绕给定的样本随机地对输入进行采样得到。目前，各种最新的计算机视觉模型都在训练过程中采用了输入变换，以提高模型的鲁棒性和收敛速度。但是对于给定的样本，这些变换在统计上效率低下：因为他们高度相关，并且在整个模型训练过程中都是从相同的分布中得出的（？也许说的是数据增强）。我们提出了一种使该过程具有自适应性的方案，该方案利用了模型及其在训练数据的局部空间建模中的缺陷。

我们在本质上很接近难负样本挖掘，这与它密切相关：在计算机视觉中，难负样本挖掘包括识别训练集示例（或其中的一部分），这些示例被模型赋予了较低的概率，但是应该相反，为高概率。然后更改训练集分布，以强调这种难负样本，并执行下一轮模型训练。如将要描述的那样，这项工作中提出的最优化问题也可以以建设性的方式使用，类似于难负样本挖掘原理。

### 规范表述

我们用$f:\mathbb{R}^M\rightarrow\{1\dots k\}$表示一个将图像像素值向量到离散标签集的分类器。我们假设$f$有一个相关的连续损失函数$loss_f:\mathbb{R}^m\times \{1\dots k\}\rightarrow \mathbb{R}^+$。对于一个给定的$x\in \mathbb{R}^m$的图像和标签$l\in \{1\dots k\}$，我们旨在解决如下盒约束的优化问题：、

* 最小化$||r||_2$，使得
  1. $f(x+r)=l$;
  2. $x+r\in[0,1]^m$

最小化器 $r$可能不是唯一的，但是我们将其定义为从$D(x,l)$随机抽取的一个并将其用于表示$x+r$。不正式地说，$x+r$就是一张能被$f$分类为$l$的最接近的图片。显然，$D(x,f(x))=f(x)$，因此这个任务只有当$f(x)\neq l$的时候才有意义。(D是什么啊？？？)通常来说，$D(x,l)$的精准计算是一个困难的问题，

### 实验结果

